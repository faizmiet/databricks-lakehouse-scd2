# Databricks Lakehouse SCD2 Project â€” Customer Domain

This is a full end-to-end **Databricks-style Delta Lakehouse project** implemented locally using PySpark + Delta Lake.
It demonstrates a complete **Bronze â†’ Silver â†’ Gold** data engineering pipeline with **Slowly Changing Dimension Type 2 (SCD2)**.

---

## ğŸ“š Layers Overview

### âœ” Bronze  
Ingest raw JSON customer events  
Add ingestion_date  
Store as Delta Bronze table

### âœ” Silver (SCD Type 2)  
Apply change detection on customer attributes  
Maintain:
- effective_from  
- effective_to  
- is_current  
- created_ts  
- updated_ts  

### âœ” Gold  
Compute KPIs  
Example: active_customer_count

---

## ğŸ“ Project Structure

src/  
 â”œâ”€ bronze_ingest.py  
 â”œâ”€ scd_utils.py  
 â”œâ”€ silver_scd2.py  
 â””â”€ gold_kpis.py  
tests/  
data/  
requirements.txt  
README.md  

---

## ğŸ›  Local Setup

### Activate virtual environment:
source .venv/bin/activate

### Install requirements:
pip install -r requirements.txt

### Configure Delta Lake packages:
export PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.4.0 pyspark-shell"

---

## â–¶ï¸ Run the Pipeline

### 1) Bronze ingestion
python src/bronze_ingest.py

### 2) Silver SCD2 processing
python src/silver_scd2.py

### 3) Gold KPI generation
python src/gold_kpis.py

---

## ğŸ“¦ Raw JSON Input Example

Place sample JSON files in:  
`./data/raw/customer/`

Example:
{"customer_id":"c1","name":"Alice","email":"alice@example.com","address":"addr1","event_time":"2025-11-17T05:00:00"}

To simulate change:
{"customer_id":"c1","name":"Alice","email":"alice@example.com","address":"addr2","event_time":"2025-11-18T05:00:00"}

---

## ğŸ§ª Run Unit Tests
pytest -q

---

## ğŸ¤ Interview Talking Points

### Why Delta Lake?
- ACID transactions  
- MERGE support (critical for SCD2)  
- Time travel  
- Schema evolution  
- Efficient upserts  

### SCD2 Logic:
- Compare tracked columns  
- Close old record (set is_current=false, effective_to timestamp)  
- Insert new version (is_current=true)  

### Production Adaptation:
- Replace local paths with DBFS or S3  
- Use Databricks Autoloader for Bronze  
- Use Databricks Jobs to orchestrate  
- Optimize tables using OPTIMIZE and ZORDER  

---

âœ¨ This project is now fully interview-ready and portfolio-ready.
