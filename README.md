# Databricks Lakehouse SCD2 Project â€” Customer Domain

This is a full end-to-end **Databricks-style Delta Lakehouse project** implemented locally using PySpark + Delta Lake.  
It demonstrates a complete **Bronze â†’ Silver â†’ Gold** data engineering pipeline with **Slowly Changing Dimension Type 2 (SCD2)**.

## ğŸ“š Layers Overview

### âœ” Bronze
- Ingest raw JSON customer events  
- Add `ingest_date`  
- Store as Delta Bronze table  

### âœ” Silver (SCD Type 2)
- Apply change detection on customer attributes  
- Track:
  - `effective_from`  
  - `effective_to`  
  - `is_current`  
  - `created_ts`  
  - `updated_ts`  

### âœ” Gold
- Compute business KPIs  
- Example: `active_customer_count`  

## ğŸ“ Project Structure

src/
 â”œâ”€ bronze_ingest.py
 â”œâ”€ scd_utils.py
 â”œâ”€ silver_scd2.py
 â””â”€ gold_kpis.py
tests/
data/
requirements.txt
README.md

## ğŸ›  Local Setup

Activate virtual environment:
source .venv/bin/activate

Install requirements:
pip install -r requirements.txt

Configure Delta Lake packages:
export PYSPARK_SUBMIT_ARGS="--packages io.delta:delta-core_2.12:2.4.0 pyspark-shell"

## â–¶ï¸ Run the Pipeline

Bronze ingestion:
python src/bronze_ingest.py

Silver SCD2 processing:
python src/silver_scd2.py

Gold KPI generation:
python src/gold_kpis.py

## ğŸ“¦ Raw JSON Input Example

Place files under:
./data/raw/customer/

Example:
{"customer_id":"c1","name":"Alice","email":"alice@example.com","address":"addr1","event_time":"2025-11-17T05:00:00"}

Example change:
{"customer_id":"c1","name":"Alice","email":"alice@example.com","address":"addr2","event_time":"2025-11-18T05:00:00"}

## ğŸ§ª Run Unit Tests

pytest -q

